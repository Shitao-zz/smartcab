{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6>**Project 4: Train a smartcab to drive**</font> \n",
    "\n",
    "---\n",
    "\n",
    "<font size = 3>*Version 1 from Shitao Wang*</font>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "<font size = 4>**Task 1: Implement a Basic Driving Agent**</font> \n",
    "\n",
    "To begin, your only task is to get the smartcab to move around in the environment. At this point, you will not be concerned with any sort of optimal driving policy. Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "The next waypoint location relative to its current location and heading.\n",
    "The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "The current time left from the allotted deadline.\n",
    "To complete this task, simply have your driving agent choose a random action from the set of possible actions (None, 'forward', 'left', 'right') at each intersection, disregarding the input information above. Set the simulation deadline enforcement, enforce_deadline to False and observe how it performs.\n",
    "\n",
    "QUESTION: Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "The Basic driving agent takes random actions at each time and makes no attempt to learn. The resulting agent moves randomly observed by the pygame GUI. The agent reaches the destination 20 our of 100 trials with 2451 times breaking the traffic rules. Even if some of the trials evetually reach the destination, the agent takes many steps.\n",
    "The following table (Out[7]) shows some basic statistics of this basic agent.\n",
    "\n",
    "We expect the behaviour of the basic agent since there are a lot of combinations from this 48 intersections' environment. It should be noted that there are only 4 cars including our agent on the road. Our basic driving agent will cause more crashes on the road if we have more cars.\n",
    "\n",
    "*see agent_without_q_learn.py for detailed implementations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>tria_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.165000</td>\n",
       "      <td>-11.645000</td>\n",
       "      <td>27.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.112235</td>\n",
       "      <td>4.817526</td>\n",
       "      <td>9.859621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-17.500000</td>\n",
       "      <td>-23.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.500000</td>\n",
       "      <td>-14.125000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.125000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>35.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_reward  negative_reward  tria_length\n",
       "count    100.000000       100.000000   100.000000\n",
       "mean      -0.165000       -11.645000    27.200000\n",
       "std        8.112235         4.817526     9.859621\n",
       "min      -17.500000       -23.500000     5.000000\n",
       "25%       -5.500000       -14.125000    21.000000\n",
       "50%       -2.500000       -11.000000    26.000000\n",
       "75%        6.125000        -9.000000    35.250000\n",
       "max       20.500000        -1.000000    51.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "task1 = pd.read_csv('random.csv')\n",
    "task1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>**Task 2: Inform the Driving Agent**</font> \n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the smartcab and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the self.state variable. Continue with the simulation deadline enforcement enforce_deadline being set to False, and observe how your driving agent now reports the change in state as the simulation progresses.\n",
    "\n",
    "QUESTION: What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "I select light, oncoming, left, next_waypoint as the set of states.\n",
    "\n",
    "light = {'red','green'}  *(Determine whether the smartcab can move forward)*\n",
    "\n",
    "oncoming = {'forward','left','right',None} *(If the smartcab wants to turn left, it should yield the oncoming traffic)*\n",
    "\n",
    "left = {'forward','left','right',None} *(If the smartcab wants to turn right, it should yield the left forward traffic)*\n",
    "\n",
    "next_waypoint{'forward','left','right'} *(Information from the planner to guide the smartcab to the destination)*\n",
    "\n",
    "With only the information of next_waypoint, the smartcab can reach the destination within all trials with 610 times breaking the traffic rules. Basic statistics shown in Out[8].\n",
    "When applying the inputs information and contraining the action with the traffic rules, the smartcab can reach the destination within all trials with 0 times breaking the traffic rules as expected. Statistics shown in Out[9].\n",
    "\n",
    "OPTIONAL: How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "There are total $2*4*4*3 = 96$ states.\n",
    "\n",
    "I added another inputs information 'right' in my Q learning algorithm, so the total should be $2*4*4*4*3 = 384$ states. It should be noted that we do not need to know the information from the right in order to follow the traffic rules. But I added it anyway in order to use it to model a full environment for the agent. \n",
    "\n",
    "The deadline is another potential variable that we could consider. If we want to build the most comprehensive state we could include deadline in our state. However, we will greatly increase the size of our state since the dealine equals to the L1 distance between two points multiplied by a factor of 5 (check environment.py line 97). We thus choose not to include the deadline in the state.\n",
    "\n",
    "With only three other agents in this exercise, the Q matrix is likely to be sparse. The Q matrix is relative large for only 100 trials to learn. But the fact that only three other agents are in the play makes it possible to learn the optimal policy within a small number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>trial_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.300000</td>\n",
       "      <td>-6.100000</td>\n",
       "      <td>12.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.520436</td>\n",
       "      <td>4.556048</td>\n",
       "      <td>5.549593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_reward  negative_reward  trial_length\n",
       "count    100.000000       100.000000    100.000000\n",
       "mean      16.300000        -6.100000     12.300000\n",
       "std        4.520436         4.556048      5.549593\n",
       "min        4.000000       -22.000000      4.000000\n",
       "25%       13.000000        -9.000000      8.000000\n",
       "50%       16.000000        -5.500000     11.000000\n",
       "75%       19.000000        -2.750000     16.000000\n",
       "max       28.000000         0.000000     31.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task2 = pd.read_csv('planner_only.csv')\n",
    "task2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>trial_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.629286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.549593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_reward  negative_reward  trial_length\n",
       "count    100.000000            100.0    100.000000\n",
       "mean      22.400000              0.0     12.300000\n",
       "std        3.629286              0.0      5.549593\n",
       "min       18.000000              0.0      4.000000\n",
       "25%       19.500000              0.0      8.000000\n",
       "50%       22.000000              0.0     11.000000\n",
       "75%       24.500000              0.0     16.000000\n",
       "max       32.000000              0.0     31.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task22 = pd.read_csv('planner_and_traffic_rule.csv')\n",
    "task22.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>**Task 3: Implement a Q-Learning Driving Agent**</font> \n",
    "\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the smartcab will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial.\n",
    "\n",
    "QUESTION: What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "In the Q learning algorithm, the agent improves its performance by learning.\n",
    "Several tunable parameters need to be set for the Q learning algorithm to work.\n",
    "For the first try, I use learning rate alpha = 0.5, exploration rate epsilon = 0.1 and discount factor gamma = 0.9. \n",
    "\n",
    "In terms of agent behaviours, the Q learning agent does much less random action than the basic driving agent. It learns quickly to avoid violating the traffic laws and it reaches the destination much faster than the basic driving agent. The main reason for this is that Q learning algorithm encourages the actions that lead to some rewards while discourages the actions that lead to some penalties.\n",
    "A basic observation at the beginning of the trials is that the agent receives a lot of none actions. In order to improve this, I could propose three approaches:\n",
    "1) increase the exploration rate.\n",
    "2) use a higher initial values. (refers to \"optimism in the face of uncertainty\" suggested by the reviewer)\n",
    "3) add some penalty such that every action does not guide the agent to the destination is discouraged.\n",
    "\n",
    "In terms of agent statistics, at the first few trials, the total reward is very small than the later trials shown in Out[12]. After several iterations, the agent performs better. 71 out of 100 trials the smartcab reaches the destination with only 67 times breaking traffic rules which is much better than the basic drving agent, at the same time, the basic statistics of the q learning agent is also better than that of the basic driving agent shown in Out[10].\n",
    "\n",
    "\n",
    "*see the detailed implementation in agent.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>trial_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.685000</td>\n",
       "      <td>-3.205000</td>\n",
       "      <td>20.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.479816</td>\n",
       "      <td>2.618982</td>\n",
       "      <td>11.734005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.875000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>11.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>28.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_reward  negative_reward  trial_length\n",
       "count    100.000000       100.000000    100.000000\n",
       "mean      23.685000        -3.205000     20.500000\n",
       "std       10.479816         2.618982     11.734005\n",
       "min        0.000000       -16.000000      1.000000\n",
       "25%       17.875000        -4.500000     11.750000\n",
       "50%       23.000000        -2.500000     19.000000\n",
       "75%       28.500000        -1.500000     26.000000\n",
       "max       65.500000         0.000000     61.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3 = pd.read_csv('Q_learning_gamma0.9.csv')\n",
    "task3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>trial_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28.5</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>65.5</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>22.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>33.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>39.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>26.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>51.5</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>55.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>29.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>28.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>26.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>21.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>41.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>25.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>33.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>23.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>15.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>27.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>19.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>18.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>31.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>12.5</td>\n",
       "      <td>-5.5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>28.5</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>27.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>23.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>17.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>11.5</td>\n",
       "      <td>-4.5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>31.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    total_reward  negative_reward  trial_length\n",
       "0            0.0             -2.0            26\n",
       "1            3.0             -1.0            26\n",
       "2            4.5             -1.5            41\n",
       "3            8.5             -1.5            26\n",
       "4           28.0             -2.0            22\n",
       "5           24.5             -1.5            11\n",
       "6           19.0             -1.0             7\n",
       "7           12.0              0.0            36\n",
       "8           26.5             -1.5            17\n",
       "9           14.0             -4.0            26\n",
       "10          25.5             -2.5            26\n",
       "11          28.5             -3.5            19\n",
       "12          23.0             -1.0             9\n",
       "13          65.5            -10.5            61\n",
       "14          44.0             -6.0            33\n",
       "15          22.5             -1.5             9\n",
       "16          21.0             -1.0             8\n",
       "17          24.0             -2.0            15\n",
       "18           9.5              0.0             1\n",
       "19          28.0             -2.0            15\n",
       "20          33.5             -2.5            19\n",
       "21          25.5             -2.5            19\n",
       "22          25.5             -0.5             9\n",
       "23          21.5             -2.5            13\n",
       "24          29.5             -2.5            16\n",
       "25          39.0             -5.0            61\n",
       "26          14.0             -2.0            11\n",
       "27          19.5             -4.5            21\n",
       "28          31.5             -4.5            31\n",
       "29          26.0             -8.0            34\n",
       "..           ...              ...           ...\n",
       "70          51.5             -6.5            41\n",
       "71          15.0             -1.0             5\n",
       "72          55.0             -7.0            45\n",
       "73          29.5             -2.5            16\n",
       "74          28.5             -1.5            13\n",
       "75          26.0             -2.0            16\n",
       "76          21.5             -0.5             7\n",
       "77          22.0              0.0             8\n",
       "78          17.0             -1.0             5\n",
       "79          41.5             -4.5            32\n",
       "80          25.0             -5.0            31\n",
       "81          20.0             -6.0            26\n",
       "82          33.0             -5.0            25\n",
       "83          19.0             -1.0             7\n",
       "84          19.0             -1.0             7\n",
       "85          23.5             -2.5            16\n",
       "86          15.5             -0.5             5\n",
       "87          24.0              0.0             8\n",
       "88          27.5             -4.5            22\n",
       "89          19.5             -2.5            15\n",
       "90          18.0             -6.0            31\n",
       "91          31.0             -9.0            51\n",
       "92          12.5             -5.5            21\n",
       "93          28.5             -1.5            18\n",
       "94          27.0             -1.0            12\n",
       "95          23.5             -0.5            12\n",
       "96          15.0             -7.0            26\n",
       "97          17.0             -5.0            21\n",
       "98          11.5             -4.5            31\n",
       "99          31.0             -3.0            19\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>**Task 4: Improve the Q-Learning Driving Agent**</font> \n",
    "\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your smartcab:\n",
    "\n",
    "- Set the number of trials, n_trials, in the simulation to 100.\n",
    "- Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "- Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "- Adjust one or several of the above parameters and iterate this process.\n",
    "\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully.\n",
    "\n",
    "QUESTION: Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "The tuning parameter I select is the discount factor gamma from [0.1,0.3,0.5,0.7,0.9].\n",
    "The other two parameters, alpha and epsilon are fixed at 0.5 and 0.1, respectively.\n",
    "The gamma parameter should range from 0 to 1 and the larger this value is, the more influence of the future states will be included.\n",
    "\n",
    "gamma = 0.1 provides the best performance, in which the smartcab reaches the destination 90 out of 100 trials with 36 times breaking the traffic rules. The fact that the best performance comes from the small gamma indicats weighting too much future reward may degrade the performance of the agent/\n",
    "Summary of performance for all different values of gamma is shown in the following table.\n",
    "The basic statistics of gamma = 0.1 are shown in Out[11].\n",
    "\n",
    "| gamma | destination reached | crashes| averaged trial length |\n",
    "| ------| ------ | ------ | ------ | ------ |\n",
    "|0.1|90|36|15.66|\n",
    "|0.3|90|42|15.55|\n",
    "|0.5|86|41|17.40|\n",
    "|0.7|78|43|18.83|\n",
    "|0.9|71|67|20.50|\n",
    "\n",
    "QUESTION: Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "The optimal policy for this problem can be described as that the agent reaches the destination within a very small number of steps and obey the traffic rules at the same time.\n",
    "Although there are still failures, crashes, rule breakings, the final policy is much better compared with the basic agent. The Q learning algorithm amazingly learn from the traffic rules and other information to improve the performance of the agent. For the future work, more parameters can be tuned in order to obtain a even better policy.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_reward</th>\n",
       "      <th>negative_reward</th>\n",
       "      <th>trial_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>20.625000</td>\n",
       "      <td>-0.630000</td>\n",
       "      <td>15.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.561433</td>\n",
       "      <td>0.799684</td>\n",
       "      <td>9.536977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_reward  negative_reward  trial_length\n",
       "count    100.000000       100.000000    100.000000\n",
       "mean      20.625000        -0.630000     15.660000\n",
       "std        6.561433         0.799684      9.536977\n",
       "min        0.000000        -5.000000      1.000000\n",
       "25%       18.000000        -1.000000      9.000000\n",
       "50%       21.500000        -0.500000     13.000000\n",
       "75%       24.000000         0.000000     21.000000\n",
       "max       37.000000         0.000000     49.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task4 = pd.read_csv('Q_learning_gamma0.1.csv')\n",
    "task4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
